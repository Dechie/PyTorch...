{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":99691,"sourceType":"datasetVersion","datasetId":52721}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":10.324732,"end_time":"2024-05-10T15:50:39.012511","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-05-10T15:50:28.687779","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n    \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\ninput_file = ''\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        input_file = os.path.join(dirname, filename)\n        print(input_file)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":1.108427,"end_time":"2024-05-10T15:50:33.367327","exception":false,"start_time":"2024-05-10T15:50:32.258900","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-20T12:56:32.078933Z","iopub.execute_input":"2024-05-20T12:56:32.079515Z","iopub.status.idle":"2024-05-20T12:56:32.089430Z","shell.execute_reply.started":"2024-05-20T12:56:32.079466Z","shell.execute_reply":"2024-05-20T12:56:32.088100Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Important Libraries**","metadata":{"papermill":{"duration":0.007006,"end_time":"2024-05-10T15:50:33.381845","exception":false,"start_time":"2024-05-10T15:50:33.374839","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# moore libraries\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\n\n\nimport sklearn\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\n# Label Encoding\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"papermill":{"duration":1.61585,"end_time":"2024-05-10T15:50:35.005085","exception":false,"start_time":"2024-05-10T15:50:33.389235","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-20T12:56:32.091770Z","iopub.execute_input":"2024-05-20T12:56:32.092221Z","iopub.status.idle":"2024-05-20T12:56:32.105861Z","shell.execute_reply.started":"2024-05-20T12:56:32.092186Z","shell.execute_reply":"2024-05-20T12:56:32.104665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import and Pre-process Data ","metadata":{"papermill":{"duration":0.007128,"end_time":"2024-05-10T15:50:35.020157","exception":false,"start_time":"2024-05-10T15:50:35.013029","status":"completed"},"tags":[]}},{"cell_type":"code","source":"data = pd.read_csv(input_file)\noriginal_data = data.copy() # needed for later\n\ndata.head()","metadata":{"papermill":{"duration":0.086678,"end_time":"2024-05-10T15:50:35.114377","exception":false,"start_time":"2024-05-10T15:50:35.027699","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-20T12:56:32.107685Z","iopub.execute_input":"2024-05-20T12:56:32.108826Z","iopub.status.idle":"2024-05-20T12:56:32.140037Z","shell.execute_reply.started":"2024-05-20T12:56:32.108768Z","shell.execute_reply":"2024-05-20T12:56:32.139123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()\ndata.columns","metadata":{"papermill":{"duration":0.044054,"end_time":"2024-05-10T15:50:35.167033","exception":false,"start_time":"2024-05-10T15:50:35.122979","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-20T12:56:32.142194Z","iopub.execute_input":"2024-05-20T12:56:32.142854Z","iopub.status.idle":"2024-05-20T12:56:32.159988Z","shell.execute_reply.started":"2024-05-20T12:56:32.142817Z","shell.execute_reply":"2024-05-20T12:56:32.158747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"papermill":{"duration":0.008465,"end_time":"2024-05-10T15:50:35.184165","exception":false,"start_time":"2024-05-10T15:50:35.175700","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Visualizing data\n\n### **1. Non numeric data using count plot**","metadata":{"papermill":{"duration":0.011232,"end_time":"2024-05-10T15:50:35.787198","exception":false,"start_time":"2024-05-10T15:50:35.775966","status":"completed"},"tags":[]}},{"cell_type":"code","source":"non_numeric_columns = ['sex', 'activities','address','famsup','Mjob', 'Fjob', \n                       'school', 'nursery', 'Pstatus', 'paid','guardian', 'famsize', \n                       'schoolsup', 'reason', 'higher', 'internet', 'romantic',\n                       ]","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:32.161547Z","iopub.execute_input":"2024-05-20T12:56:32.162244Z","iopub.status.idle":"2024-05-20T12:56:32.169017Z","shell.execute_reply.started":"2024-05-20T12:56:32.162199Z","shell.execute_reply":"2024-05-20T12:56:32.167595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in non_numeric_columns:\n    plt.figure(figsize=(10, 6))\n    sns.countplot(x=col, hue='G3', data=data)\n    plt.title(f'Count Plot of {col} vs Target')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:32.248209Z","iopub.execute_input":"2024-05-20T12:56:32.248576Z","iopub.status.idle":"2024-05-20T12:56:41.179639Z","shell.execute_reply.started":"2024-05-20T12:56:32.248548Z","shell.execute_reply":"2024-05-20T12:56:41.178548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In these plots we see that the non-numeric features are each encoded to categories with numeric values, and there are bell (gaussian) curves for each categories\n\nThe bell curves show the frequency distribution of final grade `G3` for each category.\n\nIf the width and height of the curves are almost the same for each category for a given feature, then we can assume that that feature is not correlated with target feature and therefore does not affect the target attribute much. But if one or more of the curves are very high or wide while one or more others are very low or narrow, then we can safely assume that feature has a strong correlation with the target feature and therefore affects it greatly.\n\nIn this light, we can assume the following two features have lower correlation with target attribute `G3`, and thus we can safely drop them from our dataframe:\n- activities\n- paid\n\ntherefore let's remove them.","metadata":{}},{"cell_type":"code","source":"data = data.drop(columns=['activities', 'paid'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:41.181563Z","iopub.execute_input":"2024-05-20T12:56:41.182000Z","iopub.status.idle":"2024-05-20T12:56:41.189003Z","shell.execute_reply.started":"2024-05-20T12:56:41.181969Z","shell.execute_reply":"2024-05-20T12:56:41.187735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Encoding non-numeric values**","metadata":{}},{"cell_type":"markdown","source":"**We will use label encoding technique**","metadata":{}},{"cell_type":"code","source":"non_numeric_columns.remove('activities')\nnon_numeric_columns.remove('paid')\nlabel_encoder = LabelEncoder()\ndata_label_encoded = data.copy()\nfor column in non_numeric_columns:\n    data_label_encoded[column] = label_encoder.fit_transform(data_label_encoded[column])\n\nprint(\"\\nLabel Encoded Data:\")\nprint(data_label_encoded.head())","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:41.190612Z","iopub.execute_input":"2024-05-20T12:56:41.191007Z","iopub.status.idle":"2024-05-20T12:56:41.220375Z","shell.execute_reply.started":"2024-05-20T12:56:41.190975Z","shell.execute_reply":"2024-05-20T12:56:41.218939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data_label_encoded\n\ndata.info()","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:41.223910Z","iopub.execute_input":"2024-05-20T12:56:41.224482Z","iopub.status.idle":"2024-05-20T12:56:41.239769Z","shell.execute_reply.started":"2024-05-20T12:56:41.224437Z","shell.execute_reply":"2024-05-20T12:56:41.238408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **2. Numeric data using correlations and scatter plot**","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"### Use Scatter plot of individual features","metadata":{"papermill":{"duration":0.00953,"end_time":"2024-05-10T15:50:35.845505","exception":false,"start_time":"2024-05-10T15:50:35.835975","status":"completed"},"tags":[]}},{"cell_type":"code","source":"data_numeric_only = data.drop(columns=non_numeric_columns, axis=1)\ndata_numeric_only.columns\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:41.241320Z","iopub.execute_input":"2024-05-20T12:56:41.241757Z","iopub.status.idle":"2024-05-20T12:56:41.254701Z","shell.execute_reply.started":"2024-05-20T12:56:41.241718Z","shell.execute_reply":"2024-05-20T12:56:41.253415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Range values of numeric data columns**\n\nNeeded for later","metadata":{}},{"cell_type":"code","source":"#data_ranges = data_numeric_only.max() - data_numeric_only.min()\n\n\ndata_numeric_only.max()\ndata_numeric_only.min()\n\ndata_ranges = {}\n\nfor col in data_numeric_only:\n    data_ranges[col] = {'max': data_numeric_only[col].max(), 'min': data_numeric_only[col].min()}\n    \ndata_ranges\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:41.255815Z","iopub.execute_input":"2024-05-20T12:56:41.256154Z","iopub.status.idle":"2024-05-20T12:56:41.270608Z","shell.execute_reply.started":"2024-05-20T12:56:41.256123Z","shell.execute_reply":"2024-05-20T12:56:41.269595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Correlations of numeric values with target attribute**","metadata":{}},{"cell_type":"code","source":"correlations = data_numeric_only.drop(columns=['G3']).corrwith(data_numeric_only['G3'])\ncorrelations\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:41.271867Z","iopub.execute_input":"2024-05-20T12:56:41.272932Z","iopub.status.idle":"2024-05-20T12:56:41.293858Z","shell.execute_reply.started":"2024-05-20T12:56:41.272886Z","shell.execute_reply":"2024-05-20T12:56:41.292984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Show top k features with highest correlation with target attribute (G3)\n\nLet's chose `k = 6`\n\ntherefore 6 largest correlations","metadata":{}},{"cell_type":"code","source":"# Select top 6 features with highest correlation with G3\n# G3 is excluded from the correlation calculation\ncorrelations = data_numeric_only.drop(columns=['G3']).corrwith(data_numeric_only['G3'])\n\n# Select top-k features (excluding G3) with highest absolute correlation coefficients\ntop_k_features = correlations.abs().nlargest(6)\nprint(top_k_features)","metadata":{"papermill":{"duration":0.050833,"end_time":"2024-05-10T15:50:35.906293","exception":false,"start_time":"2024-05-10T15:50:35.855460","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-20T12:56:41.295095Z","iopub.execute_input":"2024-05-20T12:56:41.295442Z","iopub.status.idle":"2024-05-20T12:56:41.313592Z","shell.execute_reply.started":"2024-05-20T12:56:41.295412Z","shell.execute_reply":"2024-05-20T12:56:41.312802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plot Scatter plots of the 6 attributes against target variable","metadata":{}},{"cell_type":"code","source":"# plot the scatter plot graphs now\n\nplt.figure(figsize=(12, 18))\n\n\n\n# First subplot for feature1\nplt.subplot(3, 2, 1)  # 3 rows, 2 columns, subplot index 1\nplt.scatter(x=data['G2'], y=data['G3'], color='skyblue')\nplt.title('G2')\nplt.xlabel('G2')\nplt.ylabel('G3')\n\n# Second subplot for feature2\nplt.subplot(3, 2, 2)  # 3 rows, 2 columns, subplot index 2\nplt.scatter(x=data['G1'], y=data['G3'], color='skyblue')\nplt.title('G1')\nplt.xlabel('G1')\nplt.ylabel('G3')\n\n# Third subplot for feature3\nplt.subplot(3, 2, 3)  # 3 rows, 2 columns, subplot index 3\nplt.scatter(x=data['failures'], y=data['G3'], color='skyblue')\nplt.title('failures')\nplt.xlabel('failures')\nplt.ylabel('G3')\n\n# Fourth subplot for feature4\nplt.subplot(3, 2, 4)  # 3 rows, 2 columns, subplot index 4\nplt.scatter(x=data['Medu'], y=data['G3'], color='skyblue')\nplt.title('Medu')\nplt.xlabel('Medu')\nplt.ylabel('G3')\n\n# Fifth subplot for feature5\nplt.subplot(3, 2, 5)  # 3 rows, 2 columns, subplot index 5\nplt.scatter(x=data['higher'], y=data['G3'], color='skyblue')\nplt.title('higher')\nplt.xlabel('higher')\nplt.ylabel('G3')\n\n# Sixth subplot for feature6\nplt.subplot(3, 2, 6)  # 3 rows, 2 columns, subplot index 6\nplt.scatter(x=data['age'], y=data['G3'], color='skyblue')\nplt.title('age')\nplt.xlabel('age')\nplt.ylabel('G3')\n\nplt.tight_layout()  # Adjust layout to prevent overlapping\nplt.show()\n","metadata":{"papermill":{"duration":2.133105,"end_time":"2024-05-10T15:50:38.049758","exception":false,"start_time":"2024-05-10T15:50:35.916653","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-20T12:56:41.314858Z","iopub.execute_input":"2024-05-20T12:56:41.315318Z","iopub.status.idle":"2024-05-20T12:56:42.945896Z","shell.execute_reply.started":"2024-05-20T12:56:41.315288Z","shell.execute_reply":"2024-05-20T12:56:42.944864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"### Select attributes with lowest correlation to target variable\n\n**And drop them from the data frame**","metadata":{}},{"cell_type":"code","source":"lowest_k_features = correlations.abs().nsmallest(15)\n\nprint(lowest_k_features)","metadata":{"papermill":{"duration":0.013313,"end_time":"2024-05-10T15:50:38.075960","exception":false,"start_time":"2024-05-10T15:50:38.062647","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-20T12:56:42.949501Z","iopub.execute_input":"2024-05-20T12:56:42.949853Z","iopub.status.idle":"2024-05-20T12:56:42.959197Z","shell.execute_reply.started":"2024-05-20T12:56:42.949826Z","shell.execute_reply":"2024-05-20T12:56:42.958067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now Drop those columns**","metadata":{}},{"cell_type":"code","source":"data = data.drop(columns=['freetime', 'absences', 'famrel',\n                             'Walc', 'Dalc', 'health',]).copy()\n\n# new_data.to_csv('data_before_scaling.csv', index=False)\n# new_data","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:42.961073Z","iopub.execute_input":"2024-05-20T12:56:42.961872Z","iopub.status.idle":"2024-05-20T12:56:42.969775Z","shell.execute_reply.started":"2024-05-20T12:56:42.961830Z","shell.execute_reply":"2024-05-20T12:56:42.968486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.columns\ndata.info()","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:42.971890Z","iopub.execute_input":"2024-05-20T12:56:42.972371Z","iopub.status.idle":"2024-05-20T12:56:42.986394Z","shell.execute_reply.started":"2024-05-20T12:56:42.972328Z","shell.execute_reply":"2024-05-20T12:56:42.985117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## Split Data into X and y features, and then scale it.\n\n### Split data to `X` and `y`","metadata":{}},{"cell_type":"code","source":"y = data['G3'].copy()\nX = data.drop('G3', axis=1).copy()\nX","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:42.987986Z","iopub.execute_input":"2024-05-20T12:56:42.988624Z","iopub.status.idle":"2024-05-20T12:56:43.015387Z","shell.execute_reply.started":"2024-05-20T12:56:42.988563Z","shell.execute_reply":"2024-05-20T12:56:43.014285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:43.016948Z","iopub.execute_input":"2024-05-20T12:56:43.017341Z","iopub.status.idle":"2024-05-20T12:56:43.025295Z","shell.execute_reply.started":"2024-05-20T12:56:43.017310Z","shell.execute_reply":"2024-05-20T12:56:43.024089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nsns.histplot(y, kde=True, color='red')\nplt.title('grade distribution')\nplt.xlabel('G3')\nplt.ylabel('frequency')","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:43.027065Z","iopub.execute_input":"2024-05-20T12:56:43.027434Z","iopub.status.idle":"2024-05-20T12:56:43.463896Z","shell.execute_reply.started":"2024-05-20T12:56:43.027404Z","shell.execute_reply":"2024-05-20T12:56:43.462773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Scale the data \nScaling the data gives it each column in `X` a mean of 0 and standard deviation of 1\n\nwe'll use Scaler from sklearn","metadata":{}},{"cell_type":"code","source":"print(f\" x mean: {X.mean()}, x standard deviation: {X.std()}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:43.465280Z","iopub.execute_input":"2024-05-20T12:56:43.465674Z","iopub.status.idle":"2024-05-20T12:56:43.474882Z","shell.execute_reply.started":"2024-05-20T12:56:43.465634Z","shell.execute_reply":"2024-05-20T12:56:43.473710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\" y mean: {y.mean()}, y standard deviation: {y.std()}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:43.476098Z","iopub.execute_input":"2024-05-20T12:56:43.476446Z","iopub.status.idle":"2024-05-20T12:56:43.484710Z","shell.execute_reply.started":"2024-05-20T12:56:43.476414Z","shell.execute_reply":"2024-05-20T12:56:43.483617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:43.486270Z","iopub.execute_input":"2024-05-20T12:56:43.487315Z","iopub.status.idle":"2024-05-20T12:56:43.495795Z","shell.execute_reply.started":"2024-05-20T12:56:43.487280Z","shell.execute_reply":"2024-05-20T12:56:43.494573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:43.497196Z","iopub.execute_input":"2024-05-20T12:56:43.498063Z","iopub.status.idle":"2024-05-20T12:56:43.510749Z","shell.execute_reply.started":"2024-05-20T12:56:43.498004Z","shell.execute_reply":"2024-05-20T12:56:43.509458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = preprocessing.StandardScaler()\nX = scaler.fit_transform(X)\n\nX = (X - X.mean()) / X.std()\ny = (y - y.mean()) / y.std()\n\n# original_y_mean = y.mean()\n# original_y_std = y.std()","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:43.512069Z","iopub.execute_input":"2024-05-20T12:56:43.513039Z","iopub.status.idle":"2024-05-20T12:56:43.522891Z","shell.execute_reply.started":"2024-05-20T12:56:43.512985Z","shell.execute_reply":"2024-05-20T12:56:43.521955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scaler = preprocessing.StandardScaler()\n# X = scaler.fit_transform(X)\n\n# # Check if standard deviation of y is zero to avoid division by zero\n# if np.std(y) == 0:\n#     raise ValueError(\"The standard deviation of y is zero, standardization is not possible.\")\n\n# # Standardize y manually\n# y = (y - np.mean(y)) / np.std(y)\n\n# original_y_mean = np.mean(y)\n# original_y_std = np.std(y)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:43.524123Z","iopub.execute_input":"2024-05-20T12:56:43.524689Z","iopub.status.idle":"2024-05-20T12:56:43.530975Z","shell.execute_reply.started":"2024-05-20T12:56:43.524657Z","shell.execute_reply":"2024-05-20T12:56:43.530035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:43.532368Z","iopub.execute_input":"2024-05-20T12:56:43.532721Z","iopub.status.idle":"2024-05-20T12:56:43.543204Z","shell.execute_reply.started":"2024-05-20T12:56:43.532692Z","shell.execute_reply":"2024-05-20T12:56:43.542231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:43.544504Z","iopub.execute_input":"2024-05-20T12:56:43.544847Z","iopub.status.idle":"2024-05-20T12:56:43.553723Z","shell.execute_reply.started":"2024-05-20T12:56:43.544820Z","shell.execute_reply":"2024-05-20T12:56:43.552583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Split Data into training and testing, with 80%**","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=41, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:43.555351Z","iopub.execute_input":"2024-05-20T12:56:43.556012Z","iopub.status.idle":"2024-05-20T12:56:43.564136Z","shell.execute_reply.started":"2024-05-20T12:56:43.555971Z","shell.execute_reply":"2024-05-20T12:56:43.563170Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape\n\n\n# X_train_arr = X_train.values\n# type(X_train_arr)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:43.565606Z","iopub.execute_input":"2024-05-20T12:56:43.566294Z","iopub.status.idle":"2024-05-20T12:56:43.574230Z","shell.execute_reply.started":"2024-05-20T12:56:43.566263Z","shell.execute_reply":"2024-05-20T12:56:43.573159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:43.575543Z","iopub.execute_input":"2024-05-20T12:56:43.576194Z","iopub.status.idle":"2024-05-20T12:56:43.583735Z","shell.execute_reply.started":"2024-05-20T12:56:43.576152Z","shell.execute_reply":"2024-05-20T12:56:43.582869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Training PyTorch Model**\n\nOur data has 24 columns, so our model's first layer would have 24 input nodes.\n\nLet's have 2 hidden layers with 36 nodes, 1 more hidden layer with 12 nodes, and 1 output layer.","metadata":{}},{"cell_type":"code","source":"class NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.layer1 = nn.Linear(24, 24)\n        # 17 inputs and 17 outputs\n        self.layer2 = nn.Linear(24, 36)\n        #  17 inputs and 8 outputs\n        self.layer3 = nn.Linear(36, 24)\n        self.layer4 = nn.Linear(24, 12)\n        self.output = nn.Linear(12, 1)\n        \n    # define feed-forward function\n    # it takes a feature vector\n    def forward(self, x):\n        # we use F from the functional package\n        # we use it to pass relu function to the layers\n        # now hidden layers have RELU activation function\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.relu(self.layer3(x))\n        x = F.relu(self.layer4(x))\n#         x = self.dropout(x)\n        x = self.output(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:43.585339Z","iopub.execute_input":"2024-05-20T12:56:43.585982Z","iopub.status.idle":"2024-05-20T12:56:43.595354Z","shell.execute_reply.started":"2024-05-20T12:56:43.585942Z","shell.execute_reply":"2024-05-20T12:56:43.594306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tweaking a little bit:\n\n- initialize custom weights\n- gradient clipping\n\ngradient clipping will be done later, during training stage.","metadata":{}},{"cell_type":"code","source":"# initialize weights\n# def init_weights(m):\n#     if type(m) == nn.Linear:\n#         torch.nn.init.xavier_uniform_(m.weight)\n#         m.bias.data.fill_(0.01)\n\n# neuralNet.apply(init_weights)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:43.603530Z","iopub.execute_input":"2024-05-20T12:56:43.604140Z","iopub.status.idle":"2024-05-20T12:56:43.608671Z","shell.execute_reply.started":"2024-05-20T12:56:43.604095Z","shell.execute_reply":"2024-05-20T12:56:43.607761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's visualize our model:\nlet's see the parameter sizes of each layer","metadata":{}},{"cell_type":"code","source":"neuralNet = NeuralNetwork()\n\nparam_list = list(neuralNet.parameters())\n\nfor i in range(len(param_list)):\n    print(param_list[i].shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:43.610227Z","iopub.execute_input":"2024-05-20T12:56:43.610889Z","iopub.status.idle":"2024-05-20T12:56:43.621896Z","shell.execute_reply.started":"2024-05-20T12:56:43.610849Z","shell.execute_reply":"2024-05-20T12:56:43.620799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Convert the X and y datas into Tensors**","metadata":{}},{"cell_type":"code","source":"# X_train_array = X_train.values\n# X_test_array = X_test.values\n\n# Convert NumPy array to PyTorch tensor\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n\n# Convert y_train and y_test to NumPy arrays and then to PyTorch tensors\ny_train_array = np.array(y_train)\ny_test_array = np.array(y_test)\n\ny_train_tensor = torch.tensor(y_train_array, dtype=torch.float32).reshape(-1, 1)\ny_test_tensor = torch.tensor(y_test_array, dtype=torch.float32).reshape(-1, 1)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:43.623704Z","iopub.execute_input":"2024-05-20T12:56:43.624147Z","iopub.status.idle":"2024-05-20T12:56:43.632228Z","shell.execute_reply.started":"2024-05-20T12:56:43.624108Z","shell.execute_reply":"2024-05-20T12:56:43.631084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_tensor","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:43.633317Z","iopub.execute_input":"2024-05-20T12:56:43.633609Z","iopub.status.idle":"2024-05-20T12:56:43.649249Z","shell.execute_reply.started":"2024-05-20T12:56:43.633583Z","shell.execute_reply":"2024-05-20T12:56:43.648375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_tensor","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:43.650256Z","iopub.execute_input":"2024-05-20T12:56:43.651054Z","iopub.status.idle":"2024-05-20T12:56:43.666313Z","shell.execute_reply.started":"2024-05-20T12:56:43.650997Z","shell.execute_reply":"2024-05-20T12:56:43.665003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_tensor","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:43.667734Z","iopub.execute_input":"2024-05-20T12:56:43.668185Z","iopub.status.idle":"2024-05-20T12:56:43.677261Z","shell.execute_reply.started":"2024-05-20T12:56:43.668146Z","shell.execute_reply":"2024-05-20T12:56:43.676066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test_tensor","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:43.680095Z","iopub.execute_input":"2024-05-20T12:56:43.681003Z","iopub.status.idle":"2024-05-20T12:56:43.690520Z","shell.execute_reply.started":"2024-05-20T12:56:43.680966Z","shell.execute_reply":"2024-05-20T12:56:43.689277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Gradient Descent (Optimizer) and Loss (criterion) \nWe use the `Adam` Optimizer with `MSE` loss function and `0.1` learning rate, and additional weight-decay parameter","metadata":{}},{"cell_type":"code","source":"optimizer = torch.optim.Adam(neuralNet.parameters(), lr=0.001, weight_decay=1e-5)\ncriterion = nn.MSELoss()","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:43.691669Z","iopub.execute_input":"2024-05-20T12:56:43.692615Z","iopub.status.idle":"2024-05-20T12:56:43.697782Z","shell.execute_reply.started":"2024-05-20T12:56:43.692569Z","shell.execute_reply":"2024-05-20T12:56:43.696867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training the model within 100 epochs","metadata":{}},{"cell_type":"markdown","source":"### **First let's define code for training one epoch**","metadata":{}},{"cell_type":"code","source":"batch_size = 32  # Example batch size, adjust as needed\n\n# Create a DataLoader instance with the specified batch size\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n\ntest_dataset = TensorDataset(X_test_tensor, y_test_tensor)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n\n#X_test_tensor.shape\ny_test_tensor.shape\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:43.698721Z","iopub.execute_input":"2024-05-20T12:56:43.699096Z","iopub.status.idle":"2024-05-20T12:56:43.710828Z","shell.execute_reply.started":"2024-05-20T12:56:43.699065Z","shell.execute_reply":"2024-05-20T12:56:43.709460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Train Loop**","metadata":{}},{"cell_type":"code","source":"# def train_loop(dataloader, model, loss_function, optimizer):\n#     size = len(dataloader.dataset)\n    \n#     model.train()\n    \n#     for batch, (X, y) in enumerate(dataloader):\n#         # compute prediction and loss\n#         pred = model(X)\n#         loss = loss_function(pred, y)\n        \n#         # backpropagation\n#         optimizer.zero_grad()\n#         loss.backward()\n        \n#         torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n#         optimizer.step()\n#         # make the gradient zero to prevent buildup\n        \n        \n#         if batch % 100 == 0:\n#             loss, current = loss.item(), batch * batch_size + len(X)\n#             print(f\"loss: {loss:>7f} [{current:>5d}/{size:5d}]\")\ntraining_accuracies = []\ndef train_loop(current_index, dataloader, model, loss_function, optimizer):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    model.train()\n    \n    correct = 0\n    total_predictions = 0\n    total_loss = 0\n    \n    for batch, (X, y) in enumerate(dataloader):\n        # compute prediction and loss\n        pred = model(X)\n        loss = loss_function(pred, y)\n        \n        # backpropagation/\n        optimizer.zero_grad()\n        loss.backward()\n        \n        # Gradient clipping\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        \n        # Accumulate loss\n        total_loss += loss.item()\n        \n        # Calculate the number of correct predictions\n        num_matches = torch.sum(torch.round(pred) == torch.round(y)).item()\n        correct += num_matches\n        total_predictions += len(y)\n        \n        if batch % 100 == 0:\n            current = batch * len(X)\n            if current_index % 10 == 0:\n                print(f\"loss: {loss.item():>7f} [{current:>5d}/{size:>5d}]\")\n    \n    # Calculate average loss and accuracy\n    average_loss = total_loss / num_batches\n    accuracy_percentage = (correct / total_predictions) * 100\n    training_accuracies.append(accuracy_percentage)\n    \n    if current_index % 10 == 0:\n        print(f\"Train/ing Error: \\n Accuracy: {accuracy_percentage:>0.1f}%, Avg loss: {average_loss:>8f} \\n\")\n\n# Example of calling the train_loop function\n# train_loop(train_dataloader, model, loss_function, optimizer)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:43.712268Z","iopub.execute_input":"2024-05-20T12:56:43.712568Z","iopub.status.idle":"2024-05-20T12:56:43.722991Z","shell.execute_reply.started":"2024-05-20T12:56:43.712542Z","shell.execute_reply":"2024-05-20T12:56:43.721828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Test Loop**","metadata":{}},{"cell_type":"code","source":"losses = []\ntest_accuracies = []\ndef test_loop(current_index, dataloader, model, loss_function):\n    model.eval()\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    test_loss, correct, total_predictions = 0, 0, 0\n    \n    with torch.no_grad():\n        for X, y in dataloader:\n            pred = model(X)\n            # Calculate MSE loss\n            mse_loss = ((pred - y)**2).mean().item()\n            #print(f\"pred: {pred}\")\n            #print(f\"pred: {y}\")\n            \n            # Count the number of matching values within the batch\n            num_matches = torch.sum(torch.round(pred) == torch.round(y)).item()\n            \n            if current_index % 10 == 0:\n                print(f\"Number of matching values in this batch: {num_matches} out of {len(y)}\")\n            test_loss += mse_loss\n\n            # Accumulate the number of correct predictions and total predictions\n            correct += num_matches\n            total_predictions += len(y)\n\n    # Calculate average MSE loss\n    test_loss /= len(dataloader.dataset)\n    \n    losses.append(test_loss)\n    \n    # Calculate accuracy percentage\n    accuracy_percentage = (correct / total_predictions) * 100\n    test_accuracies.append(accuracy_percentage)\n    \n    if current_index % 10 == 0:\n        print(f\"Test Error: \\n Accuracy: {accuracy_percentage:>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n    \n    return pred","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:43.724359Z","iopub.execute_input":"2024-05-20T12:56:43.724710Z","iopub.status.idle":"2024-05-20T12:56:43.738784Z","shell.execute_reply.started":"2024-05-20T12:56:43.724679Z","shell.execute_reply":"2024-05-20T12:56:43.737709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## now train all epochs","metadata":{}},{"cell_type":"code","source":"print(losses)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:43.740183Z","iopub.execute_input":"2024-05-20T12:56:43.740512Z","iopub.status.idle":"2024-05-20T12:56:43.753372Z","shell.execute_reply.started":"2024-05-20T12:56:43.740484Z","shell.execute_reply":"2024-05-20T12:56:43.752218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 200\npredictions = []\nbest_loss = np.inf\npatience = 5\ncounter = 0\n\n\nfor t in range(num_epochs):\n    if t % 10 == 0:\n        print(f\"Epoch {t+1} of training \\n-------------------------------\")\n    train_loop(t, train_dataloader, neuralNet, criterion, optimizer)\n    if counter >= patience:\n        continue\n    prediction = test_loop(t, test_dataloader, neuralNet, criterion)\n    predictions.append(prediction)\n    \n    # Check if the test loss has improved\n    if losses[-1] < best_loss:\n        best_loss = losses[-1]\n        counter = 0\n    else:\n        counter += 1\n\n    \n    # If test loss hasn't improved for 'patience' epochs, stop testing\n    if counter >= patience:\n        # some other\n        var = 0\n    else:\n        var = 10\n        #print(\"Early stopping!\")","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:43.754599Z","iopub.execute_input":"2024-05-20T12:56:43.755200Z","iopub.status.idle":"2024-05-20T12:56:48.683684Z","shell.execute_reply.started":"2024-05-20T12:56:43.755167Z","shell.execute_reply":"2024-05-20T12:56:48.682667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\n\n# Plot Losses\nplt.subplot(1, 3, 1) # 1 row, 3 columns, subplot 1\nplt.plot(losses, label='Loss')\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.title('Training Loss over Time')\nplt.legend()\n\n# Plot Accuracies\nplt.subplot(1, 3, 2) # 1 row, 3 columns, subplot 2\nplt.plot(training_accuracies, label='Training Accuracy', color='orange')\nplt.xlabel('Iteration')\nplt.ylabel('Accuracy (%)')\nplt.title('Training Accuracy over Time')\nplt.legend()\n\nplt.subplot(1, 3, 3) # 1 row, 3 columns, subplot 3\nplt.plot(test_accuracies, label='Accuracy', color='orange')\nplt.xlabel('Iteration')\nplt.ylabel('Accuracy (%)')\nplt.title('Test Accuracy over Time')\nplt.legend()\n\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:48.685136Z","iopub.execute_input":"2024-05-20T12:56:48.685484Z","iopub.status.idle":"2024-05-20T12:56:49.485486Z","shell.execute_reply.started":"2024-05-20T12:56:48.685454Z","shell.execute_reply":"2024-05-20T12:56:49.484295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:49.486879Z","iopub.execute_input":"2024-05-20T12:56:49.487315Z","iopub.status.idle":"2024-05-20T12:56:49.515645Z","shell.execute_reply.started":"2024-05-20T12:56:49.487278Z","shell.execute_reply":"2024-05-20T12:56:49.514502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\n\nplt.plot(predictions, label='Loss')\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.title('Training Loss over Time')\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:49.516779Z","iopub.execute_input":"2024-05-20T12:56:49.517118Z","iopub.status.idle":"2024-05-20T12:56:49.807965Z","shell.execute_reply.started":"2024-05-20T12:56:49.517088Z","shell.execute_reply":"2024-05-20T12:56:49.806049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### New data","metadata":{}},{"cell_type":"code","source":"X.mean()","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:49.809314Z","iopub.status.idle":"2024-05-20T12:56:49.810166Z","shell.execute_reply.started":"2024-05-20T12:56:49.809851Z","shell.execute_reply":"2024-05-20T12:56:49.809879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"original_mean = X.mean()\noriginal_std = X.std()","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:49.812267Z","iopub.status.idle":"2024-05-20T12:56:49.813433Z","shell.execute_reply.started":"2024-05-20T12:56:49.812971Z","shell.execute_reply":"2024-05-20T12:56:49.813000Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### unseen data\n\n# Mean and standard deviation for each feature\nx_mean = original_mean\nx_std = original_std\n# Number of features\nnum_features = 24\n\n# Generate random values for each feature based on normal distribution with given mean and standard deviation\nunseen_data = {}\nfor i in range(num_features):\n    feature_name = f'Feature_{i+1}'\n#     mean = x_mean[i]\n#     std = x_std[i]\n    value = np.random.normal(x_mean, x_std)\n    unseen_data[feature_name] = [value]\n\n# Convert dictionary to DataFrame\nunseen_df = pd.DataFrame(unseen_data)\n\n# Display the generated unseen data\nprint(\"Unseen Data:\")\nprint(unseen_df)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:49.815272Z","iopub.status.idle":"2024-05-20T12:56:49.816202Z","shell.execute_reply.started":"2024-05-20T12:56:49.815868Z","shell.execute_reply":"2024-05-20T12:56:49.815896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scale the unseen data using the same mean and standard deviation values as the training data\nunseen_scaled = (unseen_df.values - x_mean) / x_std\n\n# Convert the scaled unseen data to a PyTorch tensor\nunseen_tensor = torch.tensor(unseen_scaled, dtype=torch.float32)\n\n# Make predictions using your trained model\nwith torch.no_grad():\n    neuralNet.eval()  # Set the model to evaluation mode\n    predictions = neuralNet(unseen_tensor)\n\n# Convert predictions tensor to numpy array\npredictions_array = predictions.numpy()\n\n# Display the predictions\nprint(\"Predictions:\")\nprint(predictions_array)\n\n# Convert the predictions back to original scale\npredictions_unscaled = predictions_array * y.std() + y.mean()\n\n# Plot the actual training and test data along with the predictions\nplt.figure(figsize=(10, 6))\n\n# Plot training data\nplt.scatter(range(len(y_train)), y_train, color='blue', label='Training Data')\n\n# Plot test data\nplt.scatter(range(len(y_train), len(y_train) + len(y_test)), y_test, color='red', label='Test Data')\n\n# Plot unseen data predictions\nplt.scatter(len(y_train) + len(y_test), predictions_unscaled, color='green', label='Unseen Data Predictions')\n\nplt.xlabel('Sample Index')\nplt.ylabel('Target Value')\nplt.title('Comparison of Predictions on Unseen Data with Training and Test Data')\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:49.817851Z","iopub.status.idle":"2024-05-20T12:56:49.818717Z","shell.execute_reply.started":"2024-05-20T12:56:49.818415Z","shell.execute_reply":"2024-05-20T12:56:49.818443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Save the model**","metadata":{}},{"cell_type":"code","source":"torch.save(neuralNet.state_dict(), 'model.pth')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:49.820254Z","iopub.status.idle":"2024-05-20T12:56:49.821180Z","shell.execute_reply.started":"2024-05-20T12:56:49.820906Z","shell.execute_reply":"2024-05-20T12:56:49.820929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Load model**:","metadata":{}},{"cell_type":"code","source":"model = NeuralNetwork()  # Instantiate your model\nmodel.load_state_dict(torch.load('model.pth'))\nmodel.eval()  # Set the model to evaluation mode if needed\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:49.822546Z","iopub.status.idle":"2024-05-20T12:56:49.822939Z","shell.execute_reply.started":"2024-05-20T12:56:49.822758Z","shell.execute_reply":"2024-05-20T12:56:49.822775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Example new data**","metadata":{}},{"cell_type":"markdown","source":"**Look up hash-map for label encoding**","metadata":{}},{"cell_type":"code","source":"# Define the columns of the dataset after dropping certain columns\ncolumns = non_numeric_columns\n\n# Initialize a dictionary to store the string values of the encoded classes\nstring_values = {}\nclass_mappings = {}\nle = LabelEncoder()\n# Loop through each column and encode its classes\nfor column in columns:\n    original_data[column].unique()\n    \n    encoded_classes = le.fit_transform(original_data[column])\n    original_classes = list(le.classes_)\n    class_mappings[column] = {'original_classes': original_classes, 'encoded_classes': encoded_classes}\n        \n    # Create a dictionary to map encoded classes to original string values\n    encoded_to_string = {encoded: original for encoded, original in zip(le.transform(original_classes), original_classes)}\n    string_values[column] = encoded_to_string\n\nencoded_vals = {}\n# Display the original and string values of the encoded classes for each column\nfor column, mapping in class_mappings.items():\n    print(f\"Column: {column}\")\n    if column in string_values:\n        encoded_vals[column] = {encoded: string_values[column][encoded] for encoded in mapping['encoded_classes']}\n        print(\"String values of encoded classes:\", {encoded: string_values[column][encoded] for encoded in mapping['encoded_classes']})\n    print()\n    ","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:49.824171Z","iopub.status.idle":"2024-05-20T12:56:49.824545Z","shell.execute_reply.started":"2024-05-20T12:56:49.824369Z","shell.execute_reply":"2024-05-20T12:56:49.824385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reverse_encoded_vals = {}\n\nfor column, mapping in encoded_vals.items():\n    reverse_encoded_vals[column] = {v: k for k, v in mapping.items()}\n\n# Display the reverse encoded values\nreverse_encoded_vals","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:49.826794Z","iopub.status.idle":"2024-05-20T12:56:49.827313Z","shell.execute_reply.started":"2024-05-20T12:56:49.827078Z","shell.execute_reply":"2024-05-20T12:56:49.827098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import json\n# reverse_endoded_json = json.dumps(reverse_encoded_vals)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:49.828597Z","iopub.status.idle":"2024-05-20T12:56:49.830013Z","shell.execute_reply.started":"2024-05-20T12:56:49.829691Z","shell.execute_reply":"2024-05-20T12:56:49.829726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = input('give me: ')\ny = reverse_encoded_vals['sex'][x]\ny","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:49.831583Z","iopub.status.idle":"2024-05-20T12:56:49.832879Z","shell.execute_reply.started":"2024-05-20T12:56:49.832525Z","shell.execute_reply":"2024-05-20T12:56:49.832555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\n# Helper function to convert all values to standard Python types\ndef convert_to_builtin_type(obj):\n    if isinstance(obj, dict):\n        return {k: convert_to_builtin_type(v) for k, v in obj.items()}\n    elif isinstance(obj, list):\n        return [convert_to_builtin_type(v) for v in obj]\n    elif isinstance(obj, np.integer):\n        return int(obj)\n    elif isinstance(obj, np.floating):\n        return float(obj)\n    elif isinstance(obj, np.ndarray):\n        return obj.tolist()\n    else:\n        return obj\n\n\n# Convert all values to built-in Python types\nreverse_encoded_vals = convert_to_builtin_type(reverse_encoded_vals)\n\n# Encode to JSON\nreverse_encoded_json = json.dumps(reverse_encoded_vals)\n\n# Print the encoded JSON string\nprint(reverse_encoded_json)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:49.834386Z","iopub.status.idle":"2024-05-20T12:56:49.834971Z","shell.execute_reply.started":"2024-05-20T12:56:49.834671Z","shell.execute_reply":"2024-05-20T12:56:49.834706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_ranges","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:49.836500Z","iopub.status.idle":"2024-05-20T12:56:49.837083Z","shell.execute_reply.started":"2024-05-20T12:56:49.836787Z","shell.execute_reply":"2024-05-20T12:56:49.836810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for k in reverse_encoded_vals:\n    print(f\"{k}: {reverse_encoded_vals[k]}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:49.838902Z","iopub.status.idle":"2024-05-20T12:56:49.839547Z","shell.execute_reply.started":"2024-05-20T12:56:49.839261Z","shell.execute_reply":"2024-05-20T12:56:49.839286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the columns of the dataset\nog_data_columns = ['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu',\n       'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime',\n       'failures', 'schoolsup', 'famsup', 'nursery', 'higher', 'internet',\n       'romantic', 'goout', 'G1', 'G2']\nprint(len(og_data_columns))\n\n# # Define the mean and standard deviation of the original data\n\n# Mean values for each feature\nmean_data = {\n    'school': 0.116456,\n    'sex': 0.473418,\n    'age': 16.696203,\n    'address': 0.777215,\n    'famsize': 0.288608,\n    'Pstatus': 0.896203,\n    'Medu': 2.749367,\n    'Fedu': 2.521519,\n    'Mjob': 2.169620,\n    'Fjob': 2.281013,\n    'reason': 1.255696,\n    'guardian': 0.853165,\n    'traveltime': 1.448101,\n    'studytime': 2.035443,\n    'failures': 0.334177,\n    'schoolsup': 0.129114,\n    'famsup': 0.612658,\n    'nursery': 0.794937,\n    'higher': 0.949367,\n    'internet': 0.832911,\n    'romantic': 0.334177,\n    'goout': 3.108861,\n    'G1': 10.908861,\n    'G2': 10.713924\n}\n\n# Standard deviation values for each feature\nstd_data = {\n    'school': 0.321177,\n    'sex': 0.499926,\n    'age': 1.276043,\n    'address': 0.416643,\n    'famsize': 0.453690,\n    'Pstatus': 0.305384,\n    'Medu': 1.094735,\n    'Fedu': 1.088201,\n    'Mjob': 1.227473,\n    'Fjob': 0.863542,\n    'reason': 1.208236,\n    'guardian': 0.536684,\n    'traveltime': 0.697505,\n    'studytime': 0.839240,\n    'failures': 0.743651,\n    'schoolsup': 0.335751,\n    'famsup': 0.487761,\n    'nursery': 0.404260,\n    'higher': 0.219525,\n    'internet': 0.373528,\n    'romantic': 0.472300,\n    'goout': 1.113278,\n    'G1': 3.319195,\n    'G2': 3.761505\n}\n\n# Create pandas Series\nx_mean_original = pd.Series(mean_data)\nx_std_original = pd.Series(std_data)\n\n# Display the Series\n# print(\"x_mean_original:\")\n# print(x_mean_original)\n# print(\"\\nx_std_original:\")\n# print(x_std_original)\n\nfor k in reverse_encoded_vals:\n    print(f\"{k}: {reverse_encoded_vals[k]}\")\n# # Load the label encoded data\n# # data = pd.read_csv('your_label_encoded_data.csv')  # Replace with your file path\n\n# # Define a function to reverse the label encoding\ndef reverse_label_encoding(label, column):\n    le = LabelEncoder()\n    le.fit(data[column])\n    return le.inverse_transform([label])[0]\n\n# # Prompt the user to enter new data\nnew_data = {}\n# commenting out the input fields\n# for column in og_data_columns:\n#     if column in non_numeric_columns:\n#         input_val = input(f\"Enter the value for {column}: \")\n#         new_data[column] = reverse_encoded_vals[column][input_val]\n#     else:\n#         value = float(input(f\"Enter the value for {column}: \"))\n#         value = (value - x_mean_original[column]) / x_std_original[column]\n#         new_data[column] = value\n\n# # Convert the new data to a DataFrame\n# new_data_df = pd.DataFrame([new_data])\n\n# # # Reverse the standardization\n# for column in columns:\n#     new_data_df[column] = (new_data_df[column] * original_std[column]) + original_mean[column]\n\n\n# # # Print the new data\n# # print(\"\\nNew data:\")\n# new_data_df\n\n# # Now you can use the new data to make predictions using your model\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:49.842877Z","iopub.status.idle":"2024-05-20T12:56:49.843686Z","shell.execute_reply.started":"2024-05-20T12:56:49.843339Z","shell.execute_reply":"2024-05-20T12:56:49.843373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# new_data_df\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:49.845334Z","iopub.status.idle":"2024-05-20T12:56:49.846544Z","shell.execute_reply.started":"2024-05-20T12:56:49.846228Z","shell.execute_reply":"2024-05-20T12:56:49.846258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# unseen_tensor = torch.tensor(new_data_df.values, dtype=torch.float32)\n\n# # Make predictions using your trained model\n# with torch.no_grad():\n#     neuralNet.eval()  # Set the model to evaluation mode\n#     predictions = neuralNet(unseen_tensor)\n\n# # Convert predictions tensor to numpy array\n# predictions_array = predictions.numpy()\n\n# # Display the predictions\n# print(\"Predictions:\")\n# print(predictions_array)\n\n# # Convert the predictions back to original scale\n\n\n\n# #predictions_unscaled = predictions_array * original_y_std + original_y_mean\n# print(f\"unscaled prediction: {original_y_std}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:49.847883Z","iopub.status.idle":"2024-05-20T12:56:49.848460Z","shell.execute_reply.started":"2024-05-20T12:56:49.848185Z","shell.execute_reply":"2024-05-20T12:56:49.848210Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# {'age': {'max': 22, 'min': 15},\n#  'Medu': {'max': 4, 'min': 0},\n#  'Fedu': {'max': 4, 'min': 0},\n#  'traveltime': {'max': 4, 'min': 1},\n#  'studytime': {'max': 4, 'min': 1},\n#  'failures': {'max': 3, 'min': 0},\n#  'famrel': {'max': 5, 'min': 1},\n#  'freetime': {'max': 5, 'min': 1},\n#  'goout': {'max': 5, 'min': 1},\n#  'Dalc': {'max': 5, 'min': 1},\n#  'Walc': {'max': 5, 'min': 1},\n#  'health': {'max': 5, 'min': 1},\n#  'absences': {'max': 75, 'min': 0},\n#  'G1': {'max': 19, 'min': 3},\n#  'G2': {'max': 19, 'min': 0},\n#  'G3': {'max': 20, 'min': 0}}\n\n# {'sex': {'F': 0, 'M': 1},\n#  'address': {'U': 1, 'R': 0},\n#  'famsup': {'no': 0, 'yes': 1},\n#  'Mjob': {'at_home': 0, 'health': 1, 'other': 2, 'services': 3, 'teacher': 4},\n#  'Fjob': {'teacher': 4, 'other': 2, 'services': 3, 'health': 1, 'at_home': 0},\n#  'school': {'GP': 0, 'MS': 1},\n#  'nursery': {'yes': 1, 'no': 0},\n#  'Pstatus': {'A': 0, 'T': 1},\n#  'guardian': {'mother': 1, 'father': 0, 'other': 2},\n#  'famsize': {'GT3': 0, 'LE3': 1},\n#  'schoolsup': {'yes': 1, 'no': 0},\n#  'reason': {'course': 0, 'other': 2, 'home': 1, 'reputation': 3},\n#  'higher': {'yes': 1, 'no': 0},\n#  'internet': {'no': 0, 'yes': 1},\n#  'romantic': {'no': 0, 'yes': 1}}","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:49.850062Z","iopub.status.idle":"2024-05-20T12:56:49.850624Z","shell.execute_reply.started":"2024-05-20T12:56:49.850348Z","shell.execute_reply":"2024-05-20T12:56:49.850371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Define the mean and standard deviation of the original data\n","metadata":{}},{"cell_type":"code","source":"original_mean = X.mean()\noriginal_std = X.std()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:49.851892Z","iopub.status.idle":"2024-05-20T12:56:49.852458Z","shell.execute_reply.started":"2024-05-20T12:56:49.852181Z","shell.execute_reply":"2024-05-20T12:56:49.852205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"original_data.info()","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:49.854069Z","iopub.status.idle":"2024-05-20T12:56:49.854637Z","shell.execute_reply.started":"2024-05-20T12:56:49.854361Z","shell.execute_reply":"2024-05-20T12:56:49.854384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2024-05-20T12:56:49.856215Z","iopub.status.idle":"2024-05-20T12:56:49.856797Z","shell.execute_reply.started":"2024-05-20T12:56:49.856505Z","shell.execute_reply":"2024-05-20T12:56:49.856529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}